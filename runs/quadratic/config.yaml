dataset: Cifar10
model: bert
load_checkpoint_file: null
no_cuda: false
optimizer: SGD
optimizer_cosine_lr: true
optimizer_warmup_ratio: 0.05
optimizer_decay_at_epochs:
- 80
- 150
- 250
optimizer_decay_with_factor: 10.0
optimizer_learning_rate: 0.1
optimizer_momentum: 0.9
optimizer_weight_decay: 0.0001
batch_size: 100
num_epochs: 300
seed: 42
vocab_size_or_config_json_file: -1
hidden_size: 400
position_encoding_size: -1
num_hidden_layers: 6
num_attention_heads: 9
intermediate_size: 512
hidden_act: gelu
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
max_position_embeddings: 16
type_vocab_size: 2
initializer_range: 0.02
layer_norm_eps: 1.0e-12
add_positional_encoding_to_input: false
use_learned_2d_encoding: false
share_position_encoding: false
use_attention_data: false
query_positional_score: false
use_gaussian_attention: true
attention_isotropic_gaussian: true
prune_degenerated_heads: false
reset_degenerated_heads: false
fix_original_heads_position: false
fix_original_heads_weights: false
gaussian_spread_regularizer: 0.0
gaussian_init_sigma_std: 0.01
gaussian_init_mu_std: 2.0
attention_gaussian_blur_trick: false
pooling_concatenate_size: 2
pooling_use_resnet: false
only_list_parameters: false
num_keep_checkpoints: 30
plot_attention_positions: true
output_dir: runs/quadratic
